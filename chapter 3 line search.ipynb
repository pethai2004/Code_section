{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb700dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x11b005b10>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, hessian, jacobian\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import copy, timeit\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ff9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rastrigin(x, A=10): # define objective (map from n dim to 1 dim)\n",
    "    f = A * len(x) + jnp.sum(jnp.array([xi ** 2 - A * jnp.cos(2 * xi * jnp.pi) for xi in x]))\n",
    "    return f\n",
    "\n",
    "RastriginGrad = grad(Rastrigin) # use jacobian() if function map from n to m. \n",
    "#Here grad is less computational expensive that jacobian\n",
    "RastriginGradSec = jacobian(grad(Rastrigin))\n",
    "RastriginHess = hessian(Rastrigin) \n",
    "RastriginJac = jacobian(Rastrigin)\n",
    "initial_x = np.random.rand(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638faff",
   "metadata": {},
   "source": [
    "backtracking line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0890393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backtracking(xi, func, p, grads, alpha_init=1., c=0.7, low= 0.7): # perform inexact line search\n",
    "    a = alpha_init\n",
    "    while True:\n",
    "        if func(xi + a * p) <= func(xi) + c * a * jnp.dot(p, grads):\n",
    "            return a\n",
    "        else:\n",
    "            a = low * a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe80752",
   "metadata": {},
   "source": [
    "Implement Steepest descent and preconditioning\n",
    "\n",
    "The search direction is given by\n",
    "$$p_k = - \\frac{{\\nabla}f(\\textbf{x}_k)}{\\parallel{\\nabla}f(\\textbf{x}_k)\\parallel}$$\n",
    "And general update rule is given by\n",
    "$$x_{k+1} \\leftarrow x_k + \\alpha p_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b73335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 196.72731 59.103943\n",
      "1.0 132.1322 24.33731\n",
      "1.0 81.202286 13.579773\n",
      "1.0 43.10411 11.651001\n",
      "1.0 23.263014 11.14447\n",
      "1.0 12.5198555 11.008545\n",
      "1.0 7.108055 10.958191\n",
      "1.0 3.2850423 10.947052\n",
      "1.0 1.4052193 10.945282\n",
      "1.0 0.7597969 10.944672\n",
      "1.0 0.30787194 10.94458\n",
      "1.0 0.15281661 10.944565\n",
      "1.0 0.09963487 10.94455\n",
      "frist evaluation 137.48909 final evaluation 10.94455\n"
     ]
    }
   ],
   "source": [
    "theshold = 0.1\n",
    "X_k = initial_x #np.random.rand(20)\n",
    "eval_first = Rastrigin(X_k)\n",
    "alpha = 1.\n",
    "while True:\n",
    "    grad_k = RastriginGrad(X_k)\n",
    "    grad_norm_k = jnp.linalg.norm(grad_k)\n",
    "    direction_p = - grad_k / grad_norm_k\n",
    "    alpha_k = Backtracking(X_k, Rastrigin, direction_p, grad_k)\n",
    "    X_k = X_k + alpha_k * direction_p\n",
    "    alpha_k = alpha\n",
    "    print(alpha_k, grad_norm_k, Rastrigin(X_k))\n",
    "    if grad_norm_k < theshold:\n",
    "        break\n",
    "print('frist evaluation', eval_first, 'final evaluation', Rastrigin(X_k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf987de",
   "metadata": {},
   "source": [
    "Implement Newton Method \n",
    "\n",
    "The search direction is given by\n",
    "$$p_k = - \\frac{{\\nabla}f(\\textbf{x}_k)}{{\\nabla}^2f(\\textbf{x}_k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "5368a799",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 208.40935 283.82776\n",
      "1.0 155.61899 276.5726\n",
      "0.5120000000000001 125.34485 271.59433\n",
      "1.0 61.638542 268.36316\n",
      "1.0 9.004759 268.2616\n",
      "1.0 0.036696058 268.2616\n",
      "frist evaluation 280.4375 final evaluation 268.2616\n"
     ]
    }
   ],
   "source": [
    "X_k = initial_x\n",
    "eval_first = Rastrigin(X_k)\n",
    "evalx = []\n",
    "alpha = 1.\n",
    "\n",
    "while True:\n",
    "    evalx.append(X_k)\n",
    "    G2 = np.diag(1 / np.diag(RastriginHess(X_k))) #np.linalg.pinv(RastriginHess(X_k))\n",
    "    G1 = RastriginGrad(X_k) \n",
    "    direction_p =  - G2.dot(G1)\n",
    "    alpha_k = Backtracking(X_k, Rastrigin, direction_p, G1, c=0.1, low= 0.8)\n",
    "    X_k = X_k + alpha_k * direction_p\n",
    "    alpha_k = alpha_k\n",
    "    \n",
    "    grad_norm_k = jnp.linalg.norm(G1)\n",
    "    print(alpha_k, grad_norm_k, Rastrigin(X_k))\n",
    "    if grad_norm_k < theshold*0.1:\n",
    "        break\n",
    "print('frist evaluation', eval_first, 'final evaluation', Rastrigin(X_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e06a8e",
   "metadata": {},
   "source": [
    "Example on linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272166c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first eval 0.8685655\n",
      "last eval 7.566996e-13\n"
     ]
    }
   ],
   "source": [
    "def least_squares(A, b, x): #GRAD : A.T.dot(A.dot(x)-b)/len(A), HESS : #A.T.dot(A)/len(A)\n",
    "    return (0.5/len(A)) * jnp.linalg.norm(jnp.dot(A, x)-b)**2\n",
    "\n",
    "ghk = jacobian(grad(least_squares, 2), 2)\n",
    "gk = grad(least_squares ,2)\n",
    "\n",
    "Ns = 100\n",
    "Axxk = np.random.rand(1000, Ns)\n",
    "xxk = np.random.rand(Ns, )\n",
    "b = Axxk.dot(xxk)\n",
    "\n",
    "xs = np.random.rand(Ns, ) \n",
    "print('first eval', least_squares(Axxk, b, xs))\n",
    "for step in [1, 1]:\n",
    "    Hinv = np.linalg.pinv(ghk(Axxk, b, xs).reshape(Ns, Ns))\n",
    "    xs = xs - step * Hinv.dot(gk(Axxk, b, xs))\n",
    "print('last eval', least_squares(Axxk, b, xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2e44a",
   "metadata": {},
   "source": [
    "Newton's with by solving system of linear equation\n",
    "$$p_k = - \\mathbf{H}^{-1}_k{\\nabla}f(\\textbf{x}_k)$$\n",
    "$$\\mathbf{H}_k p_k = g_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4f018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 0.00000001\n",
    "def conjugate_gradient(Ax, b, max_iters=100, tol=0.001, CG_time_step=0):\n",
    "    'A is (S++)'\n",
    "    x = np.zeros_like(b)\n",
    "    r = copy.deepcopy(b)  \n",
    "    p = copy.deepcopy(r)\n",
    "    r_dot_old = np.dot(r, r)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        z = np.dot(Ax, p) # for hessian vector product use Ax(p) as a function \n",
    "        alpha = r_dot_old /( np.dot(p, z) + EPS)\n",
    "        x += alpha * p\n",
    "        r -= alpha * z\n",
    "        CG_time_step+=1\n",
    "        if tol >= np.linalg.norm(r):\n",
    "            return x #CG_time_step\n",
    "        \n",
    "        r_dot_new = np.dot(r, r)\n",
    "        p = r + (r_dot_new / (r_dot_old + EPS)) * p\n",
    "        r_dot_old = r_dot_new\n",
    "\n",
    "    return x #CG_time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29f90a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lg = grad(least_squares, 2)\n",
    "hess_lg = lambda as_, bs, xs: jacobian(grad(least_squares, 2), 2)(as_, bs, xs).reshape(Ns, Ns)\n",
    "Hessian_vector_prod = lambda f\n",
    "\n",
    "# def hvp(f, x, v):\n",
    "#     return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
    "\n",
    "def Newton_ls(func, gradf, hessf, x_init, solver=conjugate_gradient, max_iters=20):\n",
    "    x = x_init\n",
    "    alpha = 0.1 # use fix step size\n",
    "    First_eval = func(Axxk, b, x)\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        G2 = hessf(Axxk, b, x)\n",
    "        G1 = gradf(Axxk, b, x)\n",
    "        assert np.all(np.linalg.eigvals(G2) > 0), 'hessian is not possitive-definite'\n",
    "        direction_p = - solver(G2, G1)\n",
    "        x = x + alpha * direction_p\n",
    "    execution_time = timeit.default_timer() - start\n",
    "    print('frist_eval : ', First_eval, 'last_eval', func(Axxk, b, x), 'time :', execution_time)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35c8a6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frist_eval :  0.6601352 last_eval 0.00975992 time : 0.40430069499961974\n",
      "frist_eval :  0.6601352 last_eval 0.009757383 time : 0.4022046070003853\n"
     ]
    }
   ],
   "source": [
    "xs = np.random.rand(Ns, ) \n",
    "solver0 = lambda A, b: np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "#solver1 = grad least square\n",
    "x_solved = Newton_ls(least_squares, grad_lg, hess_lg, xs, solver=conjugate_gradient) # using conjugate gradient.\n",
    "x_solved = Newton_ls(least_squares, grad_lg, hess_lg, xs, solver=solver0) # using least squre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6913fb",
   "metadata": {},
   "source": [
    "Approximate hessian newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "475e89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_bgfs(func, x, gradf, hessf, max_iters=20):\n",
    "    First_eval = func(Axxk, b, x)\n",
    "    \n",
    "    I = np.eye(Ns) \n",
    "    G2_inv = I# initialize inverse H as identity\n",
    "    alpha = 0.1 #fix search distance\n",
    "    xss = []\n",
    "    xss.append(x)\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        \n",
    "        G2 = hessf(Axxk, b, x)\n",
    "        G1 = gradf(Axxk, b, x)\n",
    "        direction_p = - jnp.dot(G2_inv, G1)\n",
    "        x = x + alpha * direction_p\n",
    "        xss.append(x)\n",
    "        \n",
    "        s = xss[-1] - xss[-2]\n",
    "        y = gradf(Axxk, b, xss[-1]) - gradf(Axxk, b, xss[-2])\n",
    "        ys = jnp.dot(y, s)\n",
    "        ssT = jnp.outer(s, s)\n",
    "        \n",
    "        G2 = G2 + np.outer(y, y)/ys\n",
    "        G2 = G2 - G2.dot(ssT.dot(G2))/np.dot(s, G2.dot(s))\n",
    "\n",
    "        \n",
    "        left = I - np.outer(s, y)/ys\n",
    "        right = I - np.outer(y, s)/ys\n",
    "        G2_inv = (left.dot(G2_inv).dot(right) + ssT/ys)\n",
    "        print('eval_function : ', func(Axxk, b, x), 'grad : ', np.linalg.norm(G1))\n",
    "    last_eval = func(Axxk, b, x)\n",
    "    return x, xss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7defb9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_function :  3.7706165 grad :  8.386166\n",
      "eval_function :  3.160022 grad :  12.591924\n",
      "eval_function :  2.64849 grad :  11.334605\n",
      "eval_function :  2.2199438 grad :  10.186216\n",
      "eval_function :  1.8609171 grad :  9.138845\n",
      "eval_function :  1.5601273 grad :  8.185016\n",
      "eval_function :  1.3081187 grad :  7.3176475\n",
      "eval_function :  1.096974 grad :  6.5300703\n",
      "eval_function :  0.92005783 grad :  5.816019\n",
      "eval_function :  0.77181345 grad :  5.1696024\n",
      "eval_function :  0.6475837 grad :  4.585316\n",
      "eval_function :  0.54346925 grad :  4.058004\n",
      "eval_function :  0.45620403 grad :  3.5828629\n",
      "eval_function :  0.38305196 grad :  3.1554267\n",
      "eval_function :  0.32172215 grad :  2.7715402\n",
      "eval_function :  0.27029517 grad :  2.4273548\n",
      "eval_function :  0.22716323 grad :  2.1193056\n",
      "eval_function :  0.19098176 grad :  1.8440948\n",
      "eval_function :  0.16062327 grad :  1.5986938\n",
      "eval_function :  0.13514362 grad :  1.3803045\n"
     ]
    }
   ],
   "source": [
    "xs = np.random.rand(Ns, ) \n",
    "xs_solved = Newton_bgfs(least_squares, xs, grad_lg, hess_lg, max_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5226219b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for one dimensional visualization only !!!!!\n",
    "#evalx = np.asarray(evalx).flatten()\n",
    "#evaly = np.asarray([Rastrigin([x_i]) for x_i in evalx])\n",
    "# fig = plt.figure()\n",
    "# ax = plt.subplot(1, 1, 1)\n",
    "# data_skip = 1\n",
    "# def init_func():\n",
    "#     ax.clear()\n",
    "#     plt.title('optimizing Rastrigin function')\n",
    "#     plt.xlabel('x')\n",
    "#     plt.ylabel('RASTRIGIN')\n",
    "#     k = np.linspace(-5.14, 5.15, 10000)\n",
    "#     yy = np.asarray([Rastrigin([xi]) for xi in np.linspace(-5.14, 5.15, 10000)])\n",
    "#     plt.plot(k, yy)\n",
    "# def fram_plt(i):\n",
    "#     #ax.plot(evalx[i:  i+data_skip], evaly[i:i+data_skip], color='k')\n",
    "#     ax.scatter(evalx[i], evaly[i], marker='o', color='r')\n",
    "# animation = FuncAnimation(fig, func=fram_plt, frames=np.arange(0, len(evalx), data_skip),\n",
    "#                           init_func=init_func, interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27aa83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_solved[1]\n",
    "evalx =\n",
    "evaly = np.asarray([least_squares(Axxk, b, xi) for xi in xs_solved[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e881077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.017699  , 3.7706165 , 3.160022  , 2.64849   , 2.2199438 ,\n",
       "       1.8609171 , 1.5601273 , 1.3081187 , 1.096974  , 0.92005783,\n",
       "       0.77181345, 0.6475837 , 0.54346925, 0.45620403, 0.38305196,\n",
       "       0.32172215, 0.27029517, 0.22716323, 0.19098176, 0.16062327,\n",
       "       0.13514362], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dab2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(evaly)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0e908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
